<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-T0PEYZJE5E"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-T0PEYZJE5E');

    </script>
    <meta charset="UTF-8">
    <title>Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</title>
    <meta name="author" content="aff-seg"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="description" content=""/>
    <meta name="keywords" content=""/>
    <meta property="og:title" content="Segmenting Object Affordances: Reproducibility and Sensitivity to Scale"/>
    <meta property="og:description" content=""/>
    <meta property="og:image" content="assets/.png"/>

    <script src="js/init.js"></script>

    <link rel="stylesheet" href="css/mystyle.css">
    <link rel="stylesheet" href="css/academicons.css">
    <link rel="stylesheet" href="css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
<div>
    <h1>Segmenting Object Affordances:<br>Reproducibility and Sensitivity to Scale</h1>
    <h3> T. Apicella<sup>1,2</sup>, A. Xompero<sup>2</sup>, P. Gastaldo<sup>1</sup>, A. Cavallaro<sup>3,4</sup></h3>
    <sup>1</sup>University of Genoa, Italy; <sup>2</sup>Queen Mary University of London, United Kingdom;<br>
    <sup>3</sup>Idiap Research Institute, Switzerland; <sup>4</sup>Ecole Polytechnique Federale de Lausanne, Switzerland
    <br>
    <br>
</div>

<div class="links">
    <!-- <a href="" class="icon_publication"><i class="fa fa-file-pdf-o" style="font-size:20px;"> Paper</i></a> -->
    <a href="https://arxiv.org/abs/2409.01814" class="icon_publication"><i class="ai ai-arxiv ai-3x"
                                                                           style="font-size:20px;"> arXiv</i></a>
    <a href="https://github.com/apicis/aff-seg" class="icon_publication"><i class="fa fa-github"
                                                                            style="font-size:20px;"> Code</i></a>
    <a href="https://doi.org/10.5281/zenodo.13627870" class="icon_publication"><i class="ai ai-zenodo"
                                                                                  style="font-size:20px;"> Trained
        models</i></a>
</div>

<br>

<!--
<div class="image">
    <img src=""/>
</div>
-->

<div class="text">
    <p>
        Visual affordance segmentation identifies image regions of an object an agent can interact with.
        Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance
        segmentation task and evaluate on small-size datasets.
        However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons.
        In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop
        without occlusions and hand-held containers, to facilitate future comparisons.
        We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that
        this model is the best-performing on most testing sets of both scenarios.
        Our analysis show that models are not robust to scale variations when object resolutions differ from those in
        the training set.
    </p>
</div>


<br>
<div class="text">
    <h2>Results unoccluded setting</h2>
    <table class="center">
        <caption>Jaccard index on <a href="https://ieeexplore.ieee.org/document/7139369">UMD</a> testing set</caption>
        <thead>
        <tr>
            <th>Model name</th>
            <th>graspable</th>
            <th>cut</th>
            <th>scoop</th>
            <th>contain</th>
            <th>pound</th>
            <th>support</th>
            <th>wrap-grasp</th>
            <th>average</th>
        </tr>
        </thead>
        <tr>
            <td>CNN</td>
            <td>56.83</td>
            <td>71.71</td>
            <td>32.48</td>
            <td>82.35</td>
            <td>50.59</td>
            <td>36.59</td>
            <td>71.50</td>
            <td>57.44</td>
        </tr>
        <tr>
            <td>AffordanceNet</td>
            <td>52.43</td>
            <td>72.14</td>
            <td>32.16</td>
            <td>84.59</td>
            <td>71.20</td>
            <td>47.24</td>
            <td>83.58</td>
            <td>63.33</td>
        </tr>
        <tr>
            <td>DRNAtt</td>
            <td>59.64</td>
            <td>74.08</td>
            <td>40.33</td>
            <td>84.69</td>
            <td>71.64</td>
            <td>71.71</td>
            <td>84.44</td>
            <td>69.50</td>
        </tr>
        <tr>
            <td>M2F-AFF</td>
            <td>68.73</td>
            <td>87.43</td>
            <td>56.52</td>
            <td>85.57</td>
            <td>66.72</td>
            <td>79.42</td>
            <td>86.66</td>
            <td>75.86</td>
        </tr>
    </table>
</div>

<br>
<div class="text">
    <h2>Results hand-occluded setting</h2>
    <table class="center">
        <caption>Jaccard index on <a href="https://arxiv.org/abs/2308.11233">CHOC-AFF</a>, <a href="https://arxiv.org/abs/2308.11233">CCM-AFF</a>, <a href="https://arxiv.org/abs/2308.11233">HO-3D-AFF</a> testing sets</caption>
        <thead>
        <tr>
            <th>Testing set</th>
            <th>Model name</th>
            <th>graspable</th>
            <th>contain</th>
            <th>arm</th>
            <th>average</th>
        </tr>
        </thead>
        <tr>
            <td rowspan="7">CHOC-B</td>
        </tr>
        <tr>
            <td>RN50F</td>
            <td>93.27</td>
            <td>83.27</td>
            <td>-</td>
            <td>-</td>
        </tr>
        <tr>
            <td>RN18U</td>
            <td>93.45</td>
            <td>79.95</td>
            <td>93.24</td>
            <td>88.88</td>
        </tr>
        <tr>
            <td>DRNAtt</td>
            <td>93.63</td>
            <td>83.88</td>
            <td>94.30</td>
            <td>90.60</td>
        </tr>
        <tr>
            <td>ACANet</td>
            <td>93.88</td>
            <td>85.17</td>
            <td>93.24</td>
            <td>90.76</td>
        </tr>
        <tr>
            <td>ACANet50</td>
            <td>94.00</td>
            <td>85.57</td>
            <td>93.70</td>
            <td>91.09</td>
        </tr>
        <tr>
            <td>M2F-AFF</td>
            <td>95.48</td>
            <td>88.61</td>
            <td>95.36</td>
            <td>93.15</td>
        </tr>
        <tr class="divider">
            <td colspan="10"></td>
        </tr>
        <tr>
            <td rowspan="7">CHOC-I</td>
        </tr>
        <tr>
            <td>RN50F</td>
            <td>92.20</td>
            <td>68.73</td>
            <td>-</td>
            <td>-</td>
        </tr>
        <tr>
            <td>RN18U</td>
            <td>92.94</td>
            <td>68.04</td>
            <td>93.78</td>
            <td>84.92</td>
        </tr>
        <tr>
            <td>DRNAtt</td>
            <td>92.85</td>
            <td>66.13</td>
            <td>94.07</td>
            <td>84.35</td>
        </tr>
        <tr>
            <td>ACANet</td>
            <td>93.11</td>
            <td>69.86</td>
            <td>93.90</td>
            <td>85.62</td>
        </tr>
        <tr>
            <td>ACANet50</td>
            <td>93.37</td>
            <td>72.66</td>
            <td>94.07</td>
            <td>86.70</td>
        </tr>
        <tr>
            <td>M2F-AFF</td>
            <td>95.26</td>
            <td>77.62</td>
            <td>96.04</td>
            <td>89.64</td>
        </tr>
        <tr class="divider">
            <td colspan="10"></td>
        </tr>
        <tr>
            <td rowspan="7">HO-3D-AFF</td>
        </tr>
        <tr>
            <td>RN50F</td>
            <td>18.14</td>
            <td>73.56</td>
            <td>-</td>
            <td>-</td>
        </tr>
        <tr>
            <td>RN18U</td>
            <td>64.79</td>
            <td>78.42</td>
            <td>32.73</td>
            <td>58.64</td>
        </tr>
        <tr>
            <td>DRNAtt</td>
            <td>38.54</td>
            <td>18.25</td>
            <td>0.32 </td>
            <td>19.04</td>
        </tr>
        <tr>
            <td>ACANet</td>
            <td>73.93</td>
            <td>73.07</td>
            <td>40.00</td>
            <td>62.33</td>
        </tr>
        <tr>
            <td>ACANet50</td>
            <td>58.40</td>
            <td>64.43</td>
            <td>39.36</td>
            <td>54.06</td>
        </tr>
        <tr>
            <td>M2F-AFF</td>
            <td>37.35</td>
            <td>65.24</td>
            <td>34.10</td>
            <td>45.56</td>
        </tr>
        <tr class="divider">
            <td colspan="10"></td>
        </tr>
        <tr>
            <td rowspan="7">CCM-AFF</td>
        </tr>
        <tr>
            <td>RN50F</td>
            <td>6.09</td>
            <td>10.61</td>
            <td>-</td>
            <td>-</td>
        </tr>
        <tr>
            <td>RN18U</td>
            <td>13.20</td>
            <td>22.28</td>
            <td>27.68</td>
            <td>21.05</td>
        </tr>
        <tr>
            <td>DRNAtt</td>
            <td>6.35</td>
            <td>0.00</td>
            <td>0.23</td>
            <td>2.19</td>
        </tr>
        <tr>
            <td>ACANet</td>
            <td>10.06</td>
            <td>25.83</td>
            <td>31.00</td>
            <td>22.30</td>
        </tr>
        <tr>
            <td>ACANet50</td>
            <td>8.12 </td>
            <td>17.43</td>
            <td>32.54</td>
            <td>19.36</td>
        </tr>
        <tr>
            <td>M2F-AFF</td>
            <td>30.49</td>
            <td>44.27</td>
            <td>53.32</td>
            <td>42.69</td>
        </tr>
    </table>
</div>

<br>
<div class="text">
    <h2>Available models</h2>
    <table class="center">
        <thead>
        <tr>
            <th>Model name</th>
            <th>UMD</th>
            <th>CHOC-AFF</th>
            <!--<th scope="col">Description</th>-->
        </tr>
        </thead>
        <tr>
            <td><a href="https://ieeexplore.ieee.org/document/7759429"> CNN</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/UMD_CNN.zip?download=1">.zip</a></td>
            <td></td>
            <!--<td>CNN is based on an encoder-decoder architecture to segment affordances.</td>-->
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1709.07326"> AffordanceNet</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/UMD_AffNet.zip?download=1">.zip</a></td>
            <td></td>
            <!--<td>AffordanceNet is a two-stage method that detects the object and segments affordances.</td>-->
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/2308.11233"> ACANet</a></td>
            <td></td>
            <td><a href="https://zenodo.org/records/8364197/files/ACANet.zip?download=1">.zip</a></td>
            <!--<td> ACANet separately segments object and hand regions, using these masks to weigh the feature maps learnt
                in a third branch for the final affordance segmentation.
            </td>-->
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/2308.11233">ACANet50</a></td>
            <td></td>
            <td><a href="https://zenodo.org/records/13627870/files/CHOC-AFF_ACANet50.zip?download=1">.zip</a>
            </td>
            <!--<td>ACANet with ResNet-50</td>-->
        </tr>
        <tr>
            <td><a href="https://ieeexplore.ieee.org/document/9190733"> RN50-F</a></td>
            <td></td>
            <td><a href="https://zenodo.org/records/13627870/files/CHOC-AFF_RN50F.zip?download=1">.zip</a></td>
            <!--<td>RN50-F uses a ResNet-50 encoder with a pyramid scene parsing module to segment only the object
                affordance graspable and contain.
            </td>-->
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/1505.04597"> ResNet18-UNet</a></td>
            <td></td>
            <td><a href="https://zenodo.org/records/13627870/files/CHOC-AFF_RN18U.zip?download=1">.zip</a></td>
            <!--<td>UNet-like model that gradually down-sample feature maps in the encoder and up-sample them in the
                decoder, preserving the information via skip connections.
            </td>-->
        </tr>
        <tr>
            <td><a href="https://www.sciencedirect.com/science/article/pii/S0925231221000278"> DRNAtt</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/UMD_DRNAtt.zip?download=1">.zip</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/CHOC-AFF_DRNAtt.zip?download=1">.zip</a></td>
            <!--<td>DRNAtt uses position and channel attention mechanisms in parallel after the feature extraction stage.
                The outputs of the attention modules are summed element-wise and the result is up-sampled through a
                learnable decoder.
            </td>-->
        </tr>
        <tr>
            <td><a href="https://arxiv.org/abs/2112.01527"> Mask2Former</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/UMD_Mask2Former.zip?download=1">.zip</a></td>
            <td><a href="https://zenodo.org/records/13627870/files/CHOC-AFF_Mask2Former.zip?download=1">.zip</a>
            </td>
            <!--<td>Mask2Former is a recent hybrid architecture that combines an encoder-decoder convolutional neural
                network with a transformer decoder to decouple the classification of classes by the segmentation,
                tackling different types of segmentation, e.g., semantic, instance, and panoptic segmentation.
                Mask2Former introduced a masking operation in the cross-attention mechanism that combines the latent
                vectors with the features extracted from the image, ignoring the pixel positions outside the object
                region.
            </td>-->
        </tr>
    </table>
</div>

<br>

<div class="text">
    <h2>Poster</h2>
    <div>
        <img src="assets/Apicella2024ECCVw_ACVR_Poster.png">
    </div>
</div>

<br>
<div>
    <h2 class="section">Reference</h2>
    <div class="text">
        If you use the code, or the models, please cite the following reference.<br><br>
        Plain text format
        <pre>
        T. Apicella, A. Xompero, P. Gastaldo, A. Cavallaro, <i>Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</i>, 
        Proceedings of the European Conference on Computer Vision Workshops, Twelfth International Workshop on Assistive Computer Vision and Robotics (ACVR),
        Milan, Italy, 29 September 2024.
        </pre>
        <br>
        Bibtex format
        <pre> 
        @InProceedings{Apicella2024ACVR_ECCVW,
            title = {Segmenting Object Affordances: Reproducibility and Sensitivity to Scale},
            author = {Apicella, T. and Xompero, A. and Gastaldo, P. and Cavallaro, A.},
            booktitle = {Proceedings of the European Conference on Computer Vision Workshops},
            note = {Twelfth International Workshop on Assistive Computer Vision and Robotics},
            address={Milan, Italy},
            month="29" # SEP,
            year = {2024},
        }
        </pre>
    </div>
</div>

<br>

<h2 class="section">Contact</h2>
<div class="text">
    If you have any further enquiries, question, or comments, please contact <a
        href="mailto:t.apicella@qmul.ac.uk">t.apicella@qmul.ac.uk</a>
    <br>
    <br>
</div>
<br><br>

<!-- Featured -->
<div id="featured">
    <div class="divider"></div>
</div>
<!-- /Featured -->

<div id="footer">
    <div class="container">
    </div>
</div>
<br><br>
</body>
</html>
