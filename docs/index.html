<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-T0PEYZJE5E"></script>
    <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-T0PEYZJE5E');

    </script>
    <meta charset="UTF-8">
    <title>Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</title>
    <!-- SEO Meta Tags -->
        <!-- <meta name="description" content="GraphNEx is a CHIST-ERA project that will contribute a graph-based framework for developing inherently explainable AI."> -->
        <meta name="author" content="GraphNEx">

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta property="og:title" content="Segmenting Object Affordances: Reproducibility and Sensitivity to Scale" />
    <meta property="og:description" content="" />
    <meta property="og:image" content="assets/.png" />

    <script src="js/init.js"></script>

    <link rel="stylesheet" href="css/mystyle.css">
    <link rel="stylesheet" href="css/academicons.css">
    <link rel="stylesheet" href="css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
<div>
    <h1>Segmenting Object Affordances:<br>Reproducibility and Sensitivity to Scale</h1>
    <h3> T. Apicella<sup>1,2</sup>, A. Xompero<sup>2</sup>, P. Gastaldo<sup>1</sup>, A. Cavallaro<sup>3,4</sup></h3>
       <sup>1</sup>University of Genoa, Italy; <sup>2</sup>Queen Mary University of London, United Kingdom;<br>
       <sup>3</sup>Idiap Research Institute, Switzerland; <sup>4</sup>Ecole Polytechnique Federale de Lausanne, Switzerland
    <br>
    <br>
</div>
<div class="links">
    <!-- <a href="" class="icon_publication"><i class="fa fa-file-pdf-o" style="font-size:20px;"> Paper</i></a> -->
    <a href="https://arxiv.org/abs/2409.01814" class="icon_publication"><i class="ai ai-arxiv ai-3x" style="font-size:20px;"> arXiv</i></a>
    <a href="https://github.com/apicis/aff-seg" class="icon_publication"><i class="fa fa-github" style="font-size:20px;"> Code</i></a>
    <a href="https://doi.org/10.5281/zenodo.13627870" class="icon_publication"><i class="ai ai-zenodo" style="font-size:20px;"> Trained models</i></a>
    <br>
</div>

<div class="image">
    <img src=""/>
</div>

<div class="text">
    <p>
    Visual affordance segmentation identifies image regions of an object an agent can interact with. 
    Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. 
    However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. 
    In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. 
    We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. 
    Our analysis show that models are not robust to scale variations when object resolutions differ from those in the training set.
</p>
</div>
    
<br>

<div class="text">
    <h2>Available models</h2>
    Models trained on hand-occluded object setting using <a href="https://arxiv.org/abs/2308.11233"> CHOC-AFF</a>:
    <ul>
    <li> <a href="https://ieeexplore.ieee.org/document/9190733"> RN50-F</a>: RN50-F uses a ResNet-50 encoder with a pyramid scene parsing module to segment only the object affordance graspable and contain.</li>
    <li> <a href="https://arxiv.org/abs/1505.04597"> ResNet18-UNet</a>: UNet-like model that gradually down-sample feature maps in the encoder and up-sample them in the decoder, preserving the information via skip connections.</li>
    <li> <a href="https://arxiv.org/abs/2308.11233"> ACANet</a>: ACANet separately segments object and hand regions, using these masks to weigh the feature maps learnt in a third branch for the final affordance segmentation. We trained also a version of ACANet with ResNet-50.</li>
    <li> <a href="https://www.sciencedirect.com/science/article/pii/S0925231221000278"> DRNAtt</a>: DRNAtt uses position and channel attention mechanisms in parallel after the feature extraction stage. The outputs of the attention modules are summed element-wise and the result is up-sampled through a learnable decoder.</li>
    <li> <a href="https://arxiv.org/abs/2112.01527"> Mask2Former</a>: Mask2Former is a recent hybrid architecture that combines an encoder-decoder convolutional neural network with a transformer decoder to decouple the classification of classes by the segmentation, tackling different types of segmentation, e.g., semantic, instance, and panoptic segmentation. Mask2Former introduced a masking operation in the cross-attention mechanism that combines the latent vectors with the features extracted from the image, ignoring the pixel positions outside the object region.</li>
    </ul>
</div>

<br>
<br>

<div>
    <h2 class="section">Reference</h2>
    <div class="text">
        If you use the code, or the models, please cite the following reference.<br><br>
        Plain text format
        <pre> 
        T. Apicella, A. Xompero, P. Gastaldo, A. Cavallaro, <i>Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</i>, 
        Proceedings of the European Conference on Computer Vision Workshops, Twelfth International Workshop on Assistive Computer Vision and Robotics (ACVR),
        Milan, Italy, 29 September 2024.
        </pre>
        <br>
        Bibtex format
        <pre> 
        @InProceedings{Apicella2024ACVR_ECCVW,
            title = {Segmenting Object Affordances: Reproducibility and Sensitivity to Scale},
            author = {Apicella, T. and Xompero, A. and Gastaldo, P. and Cavallaro, A.},
            booktitle = {Proceedings of the European Conference on Computer Vision Workshops},
            note = {Twelfth International Workshop on Assistive Computer Vision and Robotics},
            address={Milan, Italy},
            month="29" # SEP,
            year = {2024},
        }
        </pre>
    </div>
</div>
<br>
<h2 class="section">Contact</h2>
<div class="text">
    If you have any further enquiries, question, or comments, please contact <a
        href="mailto:t.apicella@qmul.ac.uk">t.apicella@qmul.ac.uk</a>
    <br>
    <br>
</div>
<br><br>

<!-- Featured -->
    <div id="featured">
        <div class="divider"></div>
    </div>
  <!-- /Featured -->

<div id="footer">
<div class="container">
</div>
</div>
<br><br>
</body>
</html>
