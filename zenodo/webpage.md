# Affordance segmentation models 
Models trained in "Segmenting Object Affordances:Reproducibility and Sensitivity to Scale" by Apicella et al.
In this paper, we provide an up-to-date benchmark by re-implementing and re-training existing methods under the same framework and setup, along with a recent architecture for semantic segmentation. Through our benchmark, the community can further investigate the affordance segmentation problem in a fair and reproducible way\footnote[1]{\scriptsize{Code and trained models are available at \url{https://apicis.github.io/aff-seg}}}. We show that models trained under the same setup have lower performance than their performance in the respective papers in the scenario of a single object on a tabletop. We assess  generalisation performance of the models by varying the object scale and using the object occupancy i.e., the ratio of pixels belonging to the object mask and the total number of pixels, as proxy for the scale. To complement the variety of architectures that tackle the affordance segmentation problem, we retrain Mask2Former\cite{cheng2022masked}, one of the most recent methods using latent vectors for segmentation. Our experiments show that Mask2Former outperforms other methods on the dataset with single object on a tabletop, and on most datasets with hand-occluded objects.

[[arXiv](...)] [[code](...)]


## Available models 
Models trained on hand-occluded object setting using [CHOC-AFF](...):
* [RN50-F](...): RN50-F uses a ResNet-50 encoder with a pyramid scene parsing module~\cite{zhao2017pyramid} to segment only the object affordances \textit{graspable} and \textit{contain}~\cite{hussain2020fpha}.  Both models implement a ResNet encoder~\cite{he2016deep}. 
* [ResNet18-UNet](...): RN18-U and ACANet~\cite{apicella2023affordance} are UNet-like models that gradually down-sample feature maps in the encoder and up-sample them in the decoder, preserving the information via skip connections~\cite{ronneberger2015u}.
* [ACANet](...): ACANet separately segments object and hand regions, using these masks to weigh the feature maps learnt in a third branch for the final affordance segmentation. Additionally, we trained a version of ACANet with ResNet-50.

Models trained on unoccluded object setting using [UMD](...):
* [AffordanceNet](...): AffordanceNet is a two-stage method that detects the object and segments affordances.
* [CNN](...): CNN is based on an encoder-decoder architecture to segment affordances. 

Models trained on both settings:
* [DRNAtt](...): DRNAtt uses position and channel attention mechanisms in parallel after the feature extraction stage~\cite{gu2021visual}. The outputs of the attention modules are summed element-wise and the result is up-sampled through a learnable decoder.
* [Mask2Former](...): Mask2Former is a recent hybrid architecture that combines an encoder-decoder convolutional neural network with a transformer decoder to decouple the classification of classes by the segmentation, tackling different types of segmentation, e.g., semantic, instance, and panoptic segmentation~\cite{cheng2022masked}. Mask2Former introduced a masking operation in the cross-attention mechanism that combines the latent vectors with the features extracted from the image, ignoring the pixel positions outside the object region. This type of processing, not considered by previous methods, can improve the learning in tasks such affordance segmentation, in which the majority of pixels belongs to the background.

## Data
* Hand-occluded object setting: [CORSMAL Hand-Occluded Containers Affordance (CHOC-AFF)](...) has $138,240$ images of 48 synthetic containers hand-held by synthetic hands rendered on top of 30 real backgrounds, and with annotations of the object affordances (\textit{graspable} and \textit{contain}) and forearm masks (\textit{arm}). 
The locations and poses of the hand-held objects vary across images, with 129,600 images of hand-held objects rendered above a table, and 8,640 images with objects rendered on a tabletop. CHOC-AFF has % splits the data in four sets 
four splits: a training set of 103,680 images, combining 26 backgrounds and 36 objects instances; a validation set of 17,280 images, using all 30 backgrounds and holding out 6 objects instances; a testing set of 13,824 images (CHOC-B), used to assess the generalisation performance to backgrounds never seen during training; and another testing set of 17,280 images (CHOC-I), to assess the generalisation performance to object instances never seen during training.

* Unoccluded object setting: [UMD](...) has 28,843 images of tools and containers, each placed on a blue rotating table, and acquired from a fixed view with the object in the center~\cite{myers2015affordance}. 
Object instances are not evenly distributed among the 17 object categories, e.g., there are 10 instances of spoon and 2 instances of pot. UMD is annotated with 7 affordance classes: \textit{grasp}, \textit{cut}, \textit{scoop}, \textit{contain}, \textit{pound}, \textit{support}, \textit{wrap-grasp}. UMD provides a pre-defined split of the dataset into training ($14,823$ images) and testing sets ($14,020$ images), holding out approximately half of object instances per category.


## Training setup
* Hand-occluded object setting: We use early stopping with a patience of $10$ epochs, setting the maximum number of epochs to $100$. The learning rate decreases by a factor of $0.5$ if there is no increase of the mean Intersection over Union in the validation set for $3$ consecutive epochs. We use the cropping window technique described in the previous work~\cite{apicella2023affordance}, resize images by a factor sampled in the interval $[1,1.5]$ and center crop the resized image with a $480 \times 480$ window. To simulate the other arm holding the object, we use horizontal flip with a probability of $0.5$. M2F-AFF uses batch size 4, learning rate $0.0001$, and an additional Gaussian noise augmentation with variance in range $[10, 100]$ to increase variability in training data.
* Unoccluded object setting: In this work, we train CNN and DRNAtt using cross-entropy loss, Adam optimiser and batch size $4$. For AffNet, we used a combination of cross-entropy and smooth-L1 losses for detection and cross-entropy for segmentation, mini-batch gradient descent with weight decay $0.001$ and batch size $2$ to fit our available GPU memory. We initialise all the mentioned architectures with a learning rate of $10^{-3}$. We train M2F-AFF with AdamW optimiser~\cite{loshchilov2017decoupled} with batch size $4$ and learning rate $10^{-4}$. We follow Mask2Former setup~\cite{cheng2022masked}, using hungarian algorithm~\cite{kuhn1955hungarian} to match the prediction from each latent vector with the corresponding annotation. We minimize the linear combination among the cross-entropy for classification $L_{cls}$, with the binary dice loss $L_{dice}$ and the binary cross-entropy $L_{ce}$ for segmentation. Losses are weighted using hyperparameters  $\lambda_{ce}$, $\lambda_{dice}$, and $\lambda_{cls}$. For all models, we decrease the learning rate by a factor of $0.5$, if there is no increase of the mean Intersection over Union in the validation set for $3$ consecutive epochs. We set the maximum number of epochs to $100$. We also use early stopping with a patience of $5$ epochs to reduce over-fitting. We use flipping with a probability of $0.5$, scaling by randomly sampling the scale factor in the interval $[1, 1.5]$ and center-cropping to simulate a zoom-in effect, color jitter with brightness, contrast, and saturation sampled randomly in the interval $[0.9, 1.1]$, and hue sampled randomly in the interval $[-0.1, 0.1]$. These augmentations allow to increase variability in the training set. We initialise all backbones with weights trained on ImageNet~\cite{deng2009imagenet}.

## Performance measures

